## here i am going to build an AI Application
## The job of AI appication is to generate best tweets 
## This application will be powered by large language models
## in this user will ask the system to make a tweet based on any topic or any condition
## first agent of this system will generate the tweet
## after generating the tweet this system will decide either i need to send it to user
## or we can make any improvement
## if the first system thinks there is any scope of improvements ,then it will send the tweet to another agent
## that is critique agent
## this critique agent based on the tweet generated by first agent will give report 
## that report will be sent to the first agent
## based on this report the first agent will generate new report and this cyle will continue until we get the best tweet
## after we get best tweet the first agent will send the tweet to user

## This particular task is achieved using Langgraph framework


## first of all i will build these two agents using langchain expression language(LCEL)

import os
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
load_dotenv() ## activating up all the secret variables
from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder

llm = ChatOpenAI(model = "gpt-3.5-turbo")


generation_prompt = [
    ("system","You are expert in the tweet generation"
    "based on the user input and requirements also considering up all the conditions"
    "you need to generate tweets .user will also provide you the critiques"
    "By taking these critiques in a positive way.You need to make better version of previous tweet"),
    MessagesPlaceholder(variable_name="messages")
]

reflection_prompt = [
("system","You are expert in making critque on the tweet provided to you"
"Based on tweet ,you need to give all positive and negative critiques after"
"analyzing the tweet in depth.You need to think rationally and logically"),
MessagesPlaceholder(variable_name="messages")

]

generation_chain = generation_prompt | llm
reflection_chain = reflection_prompt | llm

## so finally i have created both the chains 
## generation chain and reflection chain




